{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepvariant\n",
    "\n",
    "Notebook for running DeepVariant training on K8s\n",
    "\n",
    "This is a work in progress\n",
    "\n",
    "This notebook assumes you have followed the README in order to setup your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables\n",
    "# The bucket on GCS where data is stored.\n",
    "import os\n",
    "import yaml\n",
    "BUCKET=\"cloud-ml-dev_jlewi_deep_variant\"\n",
    "# Where the NFS share is mounted\n",
    "NFS_MOUNT_POINT=\"/home/jovyan/deepvariant-pd/\"\n",
    "# The directory within the NFS share where data is stored\n",
    "DATA_DIR = \"deepvariant/data\"\n",
    "# The local directory where the DATA can be found.\n",
    "LOCAL_DATA_DIR=os.path.join(NFS_MOUNT_POINT, DATA_DIR)\n",
    "\n",
    "PROJECT=\"cloud-ml-dev\"\n",
    "CLUSTER=\"gke-tf-example\"\n",
    "ZONE=\"us-east1-d\"\n",
    "\n",
    "# GCS directory to use for this run\n",
    "GCS_DIR = \"gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-10 01:24:13--  https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12166338 (12M) [application/x-tar]\r\n",
      "Saving to: ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’\r\n",
      "\r\n",
      "\r",
      "          /tmp/helm   0%[                    ]       0  --.-KB/s               \r",
      "/tmp/helm-v2.7.2-li 100%[===================>]  11.60M  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2017-12-10 01:24:13 (88.3 MB/s) - ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’ saved [12166338/12166338]\r\n",
      "\r\n",
      "linux-amd64/\r\n",
      "linux-amd64/README.md\r\n",
      "linux-amd64/LICENSE\r\n",
      "linux-amd64/helm\r\n"
     ]
    }
   ],
   "source": [
    "# Setup helm\n",
    "# TODO(jlewi): I should build a Docker image with everything we need.\n",
    "!wget -O /tmp/helm-v2.7.2-linux-amd64.tar.gz https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!tar -C /tmp -xvf /tmp/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!mv /tmp/linux-amd64/helm ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Kubectl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\r\n",
      "kubeconfig entry generated for gke-tf-example.\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud --project={PROJECT} container clusters get-credentials --zone={ZONE} {CLUSTER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data onto NFS share\n",
    "\n",
    "* The data is most likely stored on GCS\n",
    "* We need to copy it to NFS makes make_examples can't read/write from NFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml-dev_jlewi_deep_variant/reference/GRCh38_Verily_v1.genome.fa.fai...\r\n",
      "/ [0 files][    0.0 B/120.3 KiB]                                                \r",
      "/ [1 files][120.3 KiB/120.3 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/120.3 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "# Copy the reference genome from gcs to our NFS share\n",
    "!mkdir -p {NFS_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa {LOCAL_DATA_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa.fai {LOCAL_DATA_DIR}/reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai...\r\n",
      "/ [0 files][    0.0 B/211.8 KiB]                                                \r",
      "/ [1 files][211.8 KiB/211.8 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/211.8 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "#!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/base_recalibration.table\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf.idx\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realignment-targets.interval_list\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bam\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/deepvariant-pd/deepvariant/data\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa.fai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n"
     ]
    }
   ],
   "source": [
    "!find {LOCAL_DATA_DIR} -name \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run make examples\n",
    "\n",
    "* We use the helm chart to run make examples on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/jovyan/deepvariant-pd/deepvariant/data/test_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# See https://stackoverflow.com/questions/21016220/is-it-possible-to-emit-valid-yaml-with-anchors-references-disabled-using-ruby\n",
    "class ExplicitDumper(yaml.SafeDumper):\n",
    "  \"\"\"A dumper that will never emit aliases.\"\"\"\n",
    "\n",
    "  def ignore_aliases(self, data):\n",
    "    return True\n",
    "\n",
    "# Create a config file that specifies the shards.\n",
    "# Mount_dir will be the directory in the pods where we mount the NFS share.\n",
    "# IT is set in the chart\n",
    "MOUNT_DIR = \"/mnt/biotensorflow\"\n",
    "config = {\n",
    "   \"reference\": os.path.join(MOUNT_DIR, DATA_DIR, \"reference/GRCh38_Verily_v1.genome.fa\"),\n",
    "    \"shards\": [\n",
    "        {\n",
    "             \"reads\": os.path.join(MOUNT_DIR, DATA_DIR, \"FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\"),\n",
    "             \"examples\": os.path.join(MOUNT_DIR, DATA_DIR, \"test_output/chr20.tfrecord.20170815.gz\"),\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# make sure the directory for the output exists \n",
    "for s in config[\"shards\"]:\n",
    "    d = s[\"examples\"].lstrip(MOUNT_DIR)\n",
    "    base_dir = os.path.dirname(d)\n",
    "    local_output_dir = os.path.join(NFS_MOUNT_POINT, base_dir)\n",
    "    if not os.path.exists(local_output_dir):\n",
    "        print(\"Creating %s\" % local_output_dir)\n",
    "        os.makedirs(local_output_dir)\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/make_examples_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   make-test\r\n",
      "LAST DEPLOYED: Sun Dec 10 01:43:32 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1/Job\r\n",
      "NAME                    DESIRED  SUCCESSFUL  AGE\r\n",
      "make-examples-akw4fg-0  1        0           0s\r\n",
      "\r\n",
      "==> v1/Pod(related)\r\n",
      "NAME                          READY  STATUS             RESTARTS  AGE\r\n",
      "make-examples-akw4fg-0-k26mz  0/1    ContainerCreating  0         0s\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=make-test ./charts/make-examples -f {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -k {local_output_dir}/chr20.tfrecord.20170815.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27327112\r\n",
      "drwxr-xr-x 2 jovyan     users             4096 Dec 10 23:14 .\r\n",
      "drwxr-xr-x 6 jovyan     users             4096 Dec 10 02:03 ..\r\n",
      "-rw-r--r-- 1 jovyan     users      26441264520 Dec 10 07:16 chr20.tfrecord.20170815\r\n",
      "-rw-r--r-- 1 4294967294 4294967294  1541678261 Dec 10 07:16 chr20.tfrecord.20170815.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la {local_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_examples = 0\n",
    "\n",
    "examples_file = os.path.join(local_output_dir, \"chr20.tfrecord.20170815\")\n",
    "for _ in tf.python_io.tf_record_iterator(examples_file):\n",
    "    num_examples += 1\n",
    "\n",
    "print(\"num_examples=%s\" % num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 12, 10, 23, 47, 25, 417133)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now.strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-9fcb89db64da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# TODO(jlewi): What name should we use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name: \"some-name\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfrecord_path: \"{examples_path}\"\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_examples: {0}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the dataset config file.\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# TODO(jlewi): We could put this on GCS \n",
    "dataset_config_rpath = os.path.join(DATA_DIR, \"experiments\", now.strftime(\"%Y%m%d_%H%M%S\"), \"dataset_config.pbtxt\")\n",
    "\n",
    "# TODO(jlewi): It would be better if we loaded up the protocol buffer definition from the DeepVariant soure repo as \n",
    "# opposed to manually writing the ASCII version directly to a file.\n",
    "local_dataset_file = os.path.join(NFS_MOUNT_POINT, dataset_config_rpath)\n",
    "\n",
    "local_dir = os.path.dirname(local_dataset_file)\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "with open(local_dataset_file, \"w\") as hf:\n",
    "    # TODO(jlewi): What name should we use\n",
    "    hf.write('name: \"some-name\"\\n')    \n",
    "    hf.write('tfrecord_path: \"{examples_path}\"\\n'.format(examples_path))\n",
    "    hf.write('num_examples: {0}\\n'.format(num_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "train_dir: gs://cloud-ml-dev_jlewi_deep_variant/models/dv2_train_gpu_0820_1748\n",
    "dataset_config: gs://cloud-ml-dev_jlewi_deep_variant/wgs_dataset.2017-07-06/wgs_training.dataset_config.pbtxt\n",
    "        \n",
    "        \n",
    "# Create a config file for the package.\n",
    "config = {\n",
    "    \"cpu_image\": \"gcr.io/deepvariant-docker/deepvariant:0.4.0\",\n",
    "    \"gpu_image\": \"gcr.io/deepvariant-docker/deepvariant_gpu:0.4.0\",\n",
    "    \"train_dir\": \"gs://cloud-ml-dev_jlewi_deep_variant/models/dv2_train_gpu_\" + now.strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"dataset_config\":\n",
    "    \"num_ps\": 2,\n",
    "    \"num_workers\": 3,\n",
    "}\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/model_train_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
