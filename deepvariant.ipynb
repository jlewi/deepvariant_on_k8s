{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepvariant\n",
    "\n",
    "Notebook for running DeepVariant training on K8s\n",
    "\n",
    "This is a work in progress\n",
    "\n",
    "This notebook assumes you have followed the README in order to setup your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables\n",
    "# The bucket on GCS where data is stored.\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "BUCKET=\"cloud-ml-dev_jlewi_deep_variant\"\n",
    "# Where the NFS share is mounted\n",
    "NFS_MOUNT_POINT=\"/home/jovyan/deepvariant-pd/\"\n",
    "# The directory within the NFS share where data is stored\n",
    "DATA_DIR = \"deepvariant/data\"\n",
    "# The local directory where the DATA can be found.\n",
    "LOCAL_DATA_DIR=os.path.join(NFS_MOUNT_POINT, DATA_DIR)\n",
    "\n",
    "PROJECT=\"cloud-ml-dev\"\n",
    "CLUSTER=\"gke-tf-example\"\n",
    "ZONE=\"us-east1-d\"\n",
    "\n",
    "# GCS directory to use for this run\n",
    "GCS_DIR = \"gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210\"\n",
    "GCS_DATASET_CONFIG = os.path.join(GCS_DIR, \"examples\", \"dataset_config.pbtxt\")\n",
    "\n",
    "# See https://stackoverflow.com/questions/21016220/is-it-possible-to-emit-valid-yaml-with-anchors-references-disabled-using-ruby\n",
    "class ExplicitDumper(yaml.SafeDumper):\n",
    "  \"\"\"A dumper that will never emit aliases.\"\"\"\n",
    "\n",
    "  def ignore_aliases(self, data):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-10 01:24:13--  https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12166338 (12M) [application/x-tar]\r\n",
      "Saving to: ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’\r\n",
      "\r\n",
      "\r",
      "          /tmp/helm   0%[                    ]       0  --.-KB/s               \r",
      "/tmp/helm-v2.7.2-li 100%[===================>]  11.60M  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2017-12-10 01:24:13 (88.3 MB/s) - ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’ saved [12166338/12166338]\r\n",
      "\r\n",
      "linux-amd64/\r\n",
      "linux-amd64/README.md\r\n",
      "linux-amd64/LICENSE\r\n",
      "linux-amd64/helm\r\n"
     ]
    }
   ],
   "source": [
    "# Setup helm\n",
    "# TODO(jlewi): I should build a Docker image with everything we need.\n",
    "!wget -O /tmp/helm-v2.7.2-linux-amd64.tar.gz https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!tar -C /tmp -xvf /tmp/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!mv /tmp/linux-amd64/helm ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Kubectl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\r\n",
      "kubeconfig entry generated for gke-tf-example.\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud --project={PROJECT} container clusters get-credentials --zone={ZONE} {CLUSTER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data onto NFS share\n",
    "\n",
    "* The data is most likely stored on GCS\n",
    "* We need to copy it to NFS makes make_examples can't read/write from NFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml-dev_jlewi_deep_variant/reference/GRCh38_Verily_v1.genome.fa.fai...\r\n",
      "/ [0 files][    0.0 B/120.3 KiB]                                                \r",
      "/ [1 files][120.3 KiB/120.3 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/120.3 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "# Copy the reference genome from gcs to our NFS share\n",
    "!mkdir -p {NFS_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa {LOCAL_DATA_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa.fai {LOCAL_DATA_DIR}/reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai...\r\n",
      "/ [0 files][    0.0 B/211.8 KiB]                                                \r",
      "/ [1 files][211.8 KiB/211.8 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/211.8 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "#!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/base_recalibration.table\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf.idx\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realignment-targets.interval_list\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bam\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/deepvariant-pd/deepvariant/data\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa.fai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n"
     ]
    }
   ],
   "source": [
    "!find {LOCAL_DATA_DIR} -name \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run make examples\n",
    "\n",
    "* We use the helm chart to run make examples on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /home/jovyan/deepvariant-pd/deepvariant/data/test_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Create a config file that specifies the shards.\n",
    "# Mount_dir will be the directory in the pods where we mount the NFS share.\n",
    "# IT is set in the chart\n",
    "MOUNT_DIR = \"/mnt/biotensorflow\"\n",
    "config = {\n",
    "   \"reference\": os.path.join(MOUNT_DIR, DATA_DIR, \"reference/GRCh38_Verily_v1.genome.fa\"),\n",
    "    \"shards\": [\n",
    "        {\n",
    "             \"reads\": os.path.join(MOUNT_DIR, DATA_DIR, \"FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\"),\n",
    "             \"examples\": os.path.join(MOUNT_DIR, DATA_DIR, \"test_output/chr20.tfrecord.20170815.gz\"),\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# make sure the directory for the output exists \n",
    "for s in config[\"shards\"]:\n",
    "    d = s[\"examples\"].lstrip(MOUNT_DIR)\n",
    "    base_dir = os.path.dirname(d)\n",
    "    local_output_dir = os.path.join(NFS_MOUNT_POINT, base_dir)\n",
    "    if not os.path.exists(local_output_dir):\n",
    "        print(\"Creating %s\" % local_output_dir)\n",
    "        os.makedirs(local_output_dir)\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/make_examples_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   make-test\r\n",
      "LAST DEPLOYED: Sun Dec 10 01:43:32 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1/Job\r\n",
      "NAME                    DESIRED  SUCCESSFUL  AGE\r\n",
      "make-examples-akw4fg-0  1        0           0s\r\n",
      "\r\n",
      "==> v1/Pod(related)\r\n",
      "NAME                          READY  STATUS             RESTARTS  AGE\r\n",
      "make-examples-akw4fg-0-k26mz  0/1    ContainerCreating  0         0s\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=make-test ./charts/make-examples -f {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -k {local_output_dir}/chr20.tfrecord.20170815.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27327112\r\n",
      "drwxr-xr-x 2 jovyan     users             4096 Dec 10 23:14 .\r\n",
      "drwxr-xr-x 6 jovyan     users             4096 Dec 10 02:03 ..\r\n",
      "-rw-r--r-- 1 jovyan     users      26441264520 Dec 10 07:16 chr20.tfrecord.20170815\r\n",
      "-rw-r--r-- 1 4294967294 4294967294  1541678261 Dec 10 07:16 chr20.tfrecord.20170815.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la {local_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_examples = 0\n",
    "\n",
    "examples_file = os.path.join(local_output_dir, \"chr20.tfrecord.20170815\")\n",
    "for _ in tf.python_io.tf_record_iterator(examples_file):\n",
    "    num_examples += 1\n",
    "\n",
    "print(\"num_examples=%s\" % num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_EXAMPLES_PATH = os.path.join(GCS_DIR, \"examples\", \"chr20.tfrecord.20170815.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jovyan/deepvariant-pd/deepvariant/data/test_output/chr20.tfrecord.20170815.gz [Content-Type=application/octet-stream]...\r\n",
      "/ [0 files][    0.0 B/  1.4 GiB]                                                \r",
      "==> NOTE: You are uploading one or more large file(s), which would run\r\n",
      "significantly faster if you enable parallel composite uploads. This\r\n",
      "feature can be enabled by editing the\r\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\r\n",
      "configuration file. However, note that if you do this large files will\r\n",
      "be uploaded as `composite objects\r\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\r\n",
      "means that any user who downloads such objects will need to have a\r\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\r\n",
      "without a compiled crcmod, computing checksums on composite objects is\r\n",
      "so slow that gsutil disables downloads of composite objects.\r\n",
      "\r\n",
      "-\r",
      "- [0 files][ 82.2 MiB/  1.4 GiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][198.5 MiB/  1.4 GiB]                                                \r",
      "/\r",
      "/ [0 files][283.1 MiB/  1.4 GiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][327.4 MiB/  1.4 GiB]                                                \r",
      "|\r",
      "/\r",
      "/ [0 files][390.1 MiB/  1.4 GiB]                                                \r",
      "-\r",
      "- [0 files][449.4 MiB/  1.4 GiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][510.5 MiB/  1.4 GiB]                                                \r",
      "/\r",
      "-\r",
      "- [0 files][543.5 MiB/  1.4 GiB]                                                \r",
      "\\\r",
      "\\ [0 files][578.3 MiB/  1.4 GiB]                                                \r",
      "|\r",
      "/\r",
      "/ [0 files][630.9 MiB/  1.4 GiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][683.5 MiB/  1.4 GiB]   46.2 MiB/s                                   \r",
      "|\r",
      "| [0 files][718.8 MiB/  1.4 GiB]   40.6 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][752.6 MiB/  1.4 GiB]   41.3 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][801.5 MiB/  1.4 GiB]   44.2 MiB/s                                   \r",
      "/\r",
      "/ [0 files][831.2 MiB/  1.4 GiB]   39.3 MiB/s                                   \r",
      "-\r",
      "- [0 files][835.8 MiB/  1.4 GiB]    4.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][955.7 MiB/  1.4 GiB]   24.1 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][  1.0 GiB/  1.4 GiB]   97.5 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][  1.1 GiB/  1.4 GiB]   78.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][  1.1 GiB/  1.4 GiB]   65.2 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][  1.1 GiB/  1.4 GiB]   58.0 MiB/s                                   \r",
      "|\r",
      "| [0 files][  1.1 GiB/  1.4 GiB]   37.7 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][  1.2 GiB/  1.4 GiB]   30.5 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][  1.2 GiB/  1.4 GiB]   36.8 MiB/s                                   \r",
      "/\r",
      "/ [0 files][  1.3 GiB/  1.4 GiB]   44.3 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][  1.3 GiB/  1.4 GiB]   45.1 MiB/s                                   \r",
      "|\r",
      "| [0 files][  1.3 GiB/  1.4 GiB]      0.0 B/s                                   \r",
      "/\r",
      "-\r",
      "- [1 files][  1.4 GiB/  1.4 GiB]      0.0 B/s                                   \r",
      "\\\r",
      "\r\n",
      "Operation completed over 1 objects/1.4 GiB.                                      \r\n"
     ]
    }
   ],
   "source": [
    "# Copy the data to GCS\n",
    "!gsutil cp {local_output_dir}/chr20.tfrecord.20170815.gz {GCS_EXAMPLES_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset config file.\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# TODO(jlewi): We could put this on GCS \n",
    "dataset_config_rpath = os.path.join(DATA_DIR, \"experiments\", now.strftime(\"%Y%m%d_%H%M%S\"), \"dataset_config.pbtxt\")\n",
    "\n",
    "# TODO(jlewi): It would be better if we loaded up the protocol buffer definition from the DeepVariant soure repo as \n",
    "# opposed to manually writing the ASCII version directly to a file.\n",
    "local_dataset_file = os.path.join(NFS_MOUNT_POINT, dataset_config_rpath)\n",
    "\n",
    "local_dir = os.path.dirname(local_dataset_file)\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "    \n",
    "# If we install the appropriate GCS client libraries we can write directly to GCS    \n",
    "with open(local_dataset_file, \"w\") as hf:\n",
    "    # TODO(jlewi): What name should we use\n",
    "    hf.write('name: \"some-name\"\\n')    \n",
    "    hf.write('tfrecord_path: \"{0}\"\\n'.format(GCS_EXAMPLES_PATH))\n",
    "    hf.write('num_examples: {0}\\n'.format(num_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jovyan/deepvariant-pd/deepvariant/data/experiments/20171210_235851/dataset_config.pbtxt [Content-Type=application/octet-stream]...\r\n",
      "/ [0 files][    0.0 B/  151.0 B]                                                \r",
      "/ [1 files][  151.0 B/  151.0 B]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/151.0 B.                                      \r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {local_dataset_file} {GCS_DATASET_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"some-name\"\r\n",
      "tfrecord_path: \"gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210/examples/chr20.tfrecord.20170815.gz\"\r\n",
      "num_examples: 170538\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat {GCS_DATASET_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "        \n",
    "# Create a config file for the package.\n",
    "config = {\n",
    "    \"cpu_image\": \"gcr.io/deepvariant-docker/deepvariant:0.4.0\",\n",
    "    \"gpu_image\": \"gcr.io/deepvariant-docker/deepvariant_gpu:0.4.0\",\n",
    "    \"train_dir\": os.path.join(GCS_DIR, \"model\"),\n",
    "    \"dataset_config\": GCS_DATASET_CONFIG,\n",
    "    \"num_ps\": 1,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/model_train_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   train-dv\r\n",
      "LAST DEPLOYED: Mon Dec 11 01:00:31 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1alpha1/TfJob\r\n",
      "NAME      AGE\r\n",
      "train-dv  0s\r\n",
      "\r\n",
      "\r\n",
      "NOTES:\r\n",
      "1. TODO(jlewi): Explain how to get the URl of TensorBoard.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=train-dv ./charts/dv2-train/ -f {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"train-dv\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "# To clean up the job after it finishes\n",
    "!~/helm delete --purge train-dv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
