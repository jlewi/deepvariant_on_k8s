{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepvariant\n",
    "\n",
    "Notebook for running DeepVariant training on K8s\n",
    "\n",
    "This is a work in progress\n",
    "\n",
    "This notebook assumes you have followed the README in order to setup your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables\n",
    "# The bucket on GCS where data is stored.\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "BUCKET=\"cloud-ml-dev_jlewi_deep_variant\"\n",
    "# Where the NFS share is mounted\n",
    "NFS_MOUNT_POINT=\"/home/jovyan/deepvariant-pd/\"\n",
    "# The directory within the NFS share where data is stored\n",
    "DATA_DIR = \"deepvariant/data\"\n",
    "# The local directory where the DATA can be found.\n",
    "LOCAL_DATA_DIR=os.path.join(NFS_MOUNT_POINT, DATA_DIR)\n",
    "\n",
    "PROJECT=\"cloud-ml-dev\"\n",
    "CLUSTER=\"gke-tf-example\"\n",
    "ZONE=\"us-east1-d\"\n",
    "\n",
    "# GCS directory to use for this run\n",
    "GCS_DIR = \"gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210\"\n",
    "GCS_DATASET_CONFIG = os.path.join(GCS_DIR, \"examples\", \"dataset_config.pbtxt\")\n",
    "\n",
    "# See https://stackoverflow.com/questions/21016220/is-it-possible-to-emit-valid-yaml-with-anchors-references-disabled-using-ruby\n",
    "class ExplicitDumper(yaml.SafeDumper):\n",
    "  \"\"\"A dumper that will never emit aliases.\"\"\"\n",
    "\n",
    "  def ignore_aliases(self, data):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-10 01:24:13--  https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\r\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\r\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12166338 (12M) [application/x-tar]\r\n",
      "Saving to: ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’\r\n",
      "\r\n",
      "\r",
      "          /tmp/helm   0%[                    ]       0  --.-KB/s               \r",
      "/tmp/helm-v2.7.2-li 100%[===================>]  11.60M  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2017-12-10 01:24:13 (88.3 MB/s) - ‘/tmp/helm-v2.7.2-linux-amd64.tar.gz’ saved [12166338/12166338]\r\n",
      "\r\n",
      "linux-amd64/\r\n",
      "linux-amd64/README.md\r\n",
      "linux-amd64/LICENSE\r\n",
      "linux-amd64/helm\r\n"
     ]
    }
   ],
   "source": [
    "# Setup helm\n",
    "# TODO(jlewi): I should build a Docker image with everything we need.\n",
    "!wget -O /tmp/helm-v2.7.2-linux-amd64.tar.gz https://storage.googleapis.com/kubernetes-helm/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!tar -C /tmp -xvf /tmp/helm-v2.7.2-linux-amd64.tar.gz\n",
    "!mv /tmp/linux-amd64/helm ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Kubectl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\r\n",
      "kubeconfig entry generated for gke-tf-example.\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud --project={PROJECT} container clusters get-credentials --zone={ZONE} {CLUSTER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data onto NFS share\n",
    "\n",
    "* The data is most likely stored on GCS\n",
    "* We need to copy it to NFS makes make_examples can't read/write from NFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml-dev_jlewi_deep_variant/reference/GRCh38_Verily_v1.genome.fa.fai...\r\n",
      "/ [0 files][    0.0 B/120.3 KiB]                                                \r",
      "/ [1 files][120.3 KiB/120.3 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/120.3 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "# Copy the reference genome from gcs to our NFS share\n",
    "!mkdir -p {NFS_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa {LOCAL_DATA_DIR}/reference\n",
    "!gsutil cp gs://{BUCKET}/reference/GRCh38_Verily_v1.genome.fa.fai {LOCAL_DATA_DIR}/reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai...\r\n",
      "/ [0 files][    0.0 B/211.8 KiB]                                                \r",
      "/ [1 files][211.8 KiB/211.8 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/211.8 KiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/\n",
    "!gsutil cp gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai {LOCAL_DATA_DIR}/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml-dev_jlewi_deep_variant/pfda/HG001.vcf.gz.tbi...\r\n",
      "/ [0 files][    0.0 B/  1.5 MiB]                                                \r",
      "/ [1 files][  1.5 MiB/  1.5 MiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/1.5 MiB.                                      \r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {LOCAL_DATA_DIR}/pfda\n",
    "!gsutil cp gs://{BUCKET}/pfda/HG001.vcf.gz {LOCAL_DATA_DIR}/pfda\n",
    "!gsutil cp gs://{BUCKET}/pfda/HG001.bed {LOCAL_DATA_DIR}/pfda\n",
    "!gsutil cp gs://{BUCKET}/pfda/HG001.vcf.gz.tbi {LOCAL_DATA_DIR}/pfda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/deepvariant-pd/deepvariant/data\n"
     ]
    }
   ],
   "source": [
    "print(LOCAL_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/base_recalibration.table\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/final.g.vcf.idx\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realignment-targets.interval_list\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bai\r\n",
      "gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/recalibrated.bam\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://verily-analysis-precision-fda/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/deepvariant-pd/deepvariant/data\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa.fai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/reference/GRCh38_Verily_v1.genome.fa\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bai\r\n",
      "/home/jovyan/deepvariant-pd/deepvariant/data/FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\r\n"
     ]
    }
   ],
   "source": [
    "!find {LOCAL_DATA_DIR} -name \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run make examples\n",
    "\n",
    "* We use the helm chart to run make examples on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Create a config file that specifies the shards.\n",
    "# Mount_dir will be the directory in the pods where we mount the NFS share.\n",
    "# IT is set in the chart\n",
    "MOUNT_DIR = \"/mnt/biotensorflow\"\n",
    "config = {\n",
    "   \"reference\": os.path.join(MOUNT_DIR, DATA_DIR, \"reference/GRCh38_Verily_v1.genome.fa\"),\n",
    "    \"shards\": [\n",
    "        {\n",
    "             \"reads\": os.path.join(MOUNT_DIR, DATA_DIR, \"FAKE_FLOWCELL/GRCh38/HG001-NA12878-pFDA/chr20/realigned.bam\"),\n",
    "             \"examples\": os.path.join(MOUNT_DIR, DATA_DIR, \"test_output/chr20.tfrecord.20170815.gz\"),\n",
    "        },\n",
    "    ],\n",
    "    \"regions\": \"chr20\",\n",
    "    \"truth_variants\": os.path.join(MOUNT_DIR, DATA_DIR, \"pfda/HG001.vcf.gz\"),\n",
    "    \"confident_regions\": os.path.join(MOUNT_DIR, DATA_DIR, \"pfda/HG001.bed\"),\n",
    "}\n",
    "\n",
    "\n",
    "# make sure the directory for the output exists \n",
    "for s in config[\"shards\"]:\n",
    "    d = s[\"examples\"].lstrip(MOUNT_DIR)\n",
    "    base_dir = os.path.dirname(d)\n",
    "    local_output_dir = os.path.join(NFS_MOUNT_POINT, base_dir)\n",
    "    if not os.path.exists(local_output_dir):\n",
    "        print(\"Creating %s\" % local_output_dir)\n",
    "        os.makedirs(local_output_dir)\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/make_examples_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   make-test\r\n",
      "LAST DEPLOYED: Mon Dec 11 13:37:09 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1/Job\r\n",
      "NAME                    DESIRED  SUCCESSFUL  AGE\r\n",
      "make-examples-reb4u4-0  1        0           1s\r\n",
      "\r\n",
      "==> v1/Pod(related)\r\n",
      "NAME                          READY  STATUS             RESTARTS  AGE\r\n",
      "make-examples-reb4u4-0-d9v2j  0/1    ContainerCreating  0         1s\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=make-test ./charts/make-examples -f {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"make-test\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "# Clean up the make examples job after it finishes\n",
    "!~/helm delete --purge make-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the output file if you want to rerun the experiment.\n",
    "#os.unlink(os.path.join(LOCAL_DATA_DIR, \"test_output/chr20.tfrecord.20170815.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(jlewi): Do we need to unzip it to count records? Looks like tf_record_iterator can't work with .gz files.\n",
    "!gunzip -k {local_output_dir}/chr20.tfrecord.20170815.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {local_output_dir}/tmp\n",
    "!mv {local_output_dir}/chr20.tfrecord.20170815 {local_output_dir}/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14582808\r\n",
      "drwxr-xr-x 3 jovyan     users             4096 Dec 11 20:54 .\r\n",
      "drwxr-xr-x 8 jovyan     users             4096 Dec 11 04:03 ..\r\n",
      "-rw-r--r-- 1 jovyan     users      14158527262 Dec 11 19:18 chr20.tfrecord.20170815\r\n",
      "-rw-r--r-- 1 4294967294 4294967294   774242581 Dec 11 19:18 chr20.tfrecord.20170815.gz\r\n",
      "drwxr-xr-x 2 jovyan     users             4096 Dec 11 20:54 tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la {local_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_examples=91236\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_examples = 0\n",
    "\n",
    "examples_file = os.path.join(local_output_dir, \"chr20.tfrecord.20170815\")\n",
    "for _ in tf.python_io.tf_record_iterator(examples_file):\n",
    "    num_examples += 1\n",
    "\n",
    "print(\"num_examples=%s\" % num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_EXAMPLES_PATH = os.path.join(GCS_DIR, \"examples\", \"chr20.tfrecord.20170815.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jovyan/deepvariant-pd/deepvariant/data/test_output/chr20.tfrecord.20170815.gz [Content-Type=application/octet-stream]...\r\n",
      "/ [0 files][    0.0 B/738.4 MiB]                                                \r",
      "==> NOTE: You are uploading one or more large file(s), which would run\r\n",
      "significantly faster if you enable parallel composite uploads. This\r\n",
      "feature can be enabled by editing the\r\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\r\n",
      "configuration file. However, note that if you do this large files will\r\n",
      "be uploaded as `composite objects\r\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\r\n",
      "means that any user who downloads such objects will need to have a\r\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\r\n",
      "without a compiled crcmod, computing checksums on composite objects is\r\n",
      "so slow that gsutil disables downloads of composite objects.\r\n",
      "\r\n",
      "-\r",
      "- [0 files][ 80.7 MiB/738.4 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][192.1 MiB/738.4 MiB]                                                \r",
      "/\r",
      "/ [0 files][295.2 MiB/738.4 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][382.3 MiB/738.4 MiB]                                                \r",
      "|\r",
      "/\r",
      "/ [0 files][466.6 MiB/738.4 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][551.7 MiB/738.4 MiB]                                                \r",
      "|\r",
      "| [0 files][627.8 MiB/738.4 MiB]                                                \r",
      "/\r",
      "-\r",
      "- [0 files][705.1 MiB/738.4 MiB]                                                \r",
      "\\\r",
      "\\ [1 files][738.4 MiB/738.4 MiB]                                                \r",
      "|\r",
      "\r\n",
      "Operation completed over 1 objects/738.4 MiB.                                    \r\n"
     ]
    }
   ],
   "source": [
    "# Copy the data to GCS\n",
    "!gsutil cp {local_output_dir}/chr20.tfrecord.20170815.gz {GCS_EXAMPLES_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset config file.\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# TODO(jlewi): We could put this on GCS \n",
    "dataset_config_rpath = os.path.join(DATA_DIR, \"experiments\", now.strftime(\"%Y%m%d_%H%M%S\"), \"dataset_config.pbtxt\")\n",
    "\n",
    "# TODO(jlewi): It would be better if we loaded up the protocol buffer definition from the DeepVariant soure repo as \n",
    "# opposed to manually writing the ASCII version directly to a file.\n",
    "local_dataset_file = os.path.join(NFS_MOUNT_POINT, dataset_config_rpath)\n",
    "\n",
    "local_dir = os.path.dirname(local_dataset_file)\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "    \n",
    "# If we install the appropriate GCS client libraries we can write directly to GCS    \n",
    "with open(local_dataset_file, \"w\") as hf:\n",
    "    # TODO(jlewi): What name should we use\n",
    "    hf.write('name: \"some-name\"\\n')    \n",
    "    hf.write('tfrecord_path: \"{0}\"\\n'.format(GCS_EXAMPLES_PATH))\n",
    "    hf.write('num_examples: {0}\\n'.format(num_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jovyan/deepvariant-pd/deepvariant/data/experiments/20171210_235851/dataset_config.pbtxt [Content-Type=application/octet-stream]...\r\n",
      "/ [0 files][    0.0 B/  151.0 B]                                                \r",
      "/ [1 files][  151.0 B/  151.0 B]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/151.0 B.                                      \r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {local_dataset_file} {GCS_DATASET_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"some-name\"\r\n",
      "tfrecord_path: \"gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210/examples/chr20.tfrecord.20170815.gz\"\r\n",
      "num_examples: 170538\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat {GCS_DATASET_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "train_dir = os.path.join(GCS_DIR, \"model\")\n",
    "# Create a config file for the package.\n",
    "train_config = {\n",
    "    \"cpu_image\": \"gcr.io/deepvariant-docker/deepvariant:0.4.0\",\n",
    "    \"gpu_image\": \"gcr.io/deepvariant-docker/deepvariant_gpu:0.4.0\",\n",
    "    \"train_dir\": train_dir,\n",
    "    \"dataset_config\": GCS_DATASET_CONFIG,\n",
    "    \"num_ps\": 1,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "    \n",
    "CONFIG_FILE=\"/tmp/model_train_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(train_config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_image: gcr.io/deepvariant-docker/deepvariant:0.4.0\r\n",
      "dataset_config: gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210/examples/dataset_config.pbtxt\r\n",
      "gpu_image: gcr.io/deepvariant-docker/deepvariant_gpu:0.4.0\r\n",
      "num_ps: 1\r\n",
      "num_workers: 1\r\n",
      "train_dir: gs://cloud-ml-dev_jlewi_deep_variant/experiments/2017_1210/model\r\n"
     ]
    }
   ],
   "source": [
    "!cat {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   train-dv\r\n",
      "LAST DEPLOYED: Mon Dec 11 22:04:34 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1alpha1/TfJob\r\n",
      "NAME      AGE\r\n",
      "train-dv  0s\r\n",
      "\r\n",
      "\r\n",
      "NOTES:\r\n",
      "1. TODO(jlewi): Explain how to get the URl of TensorBoard.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=train-dv ./charts/dv2-train/ -f {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"train-dv\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "# To clean up the job after it finishes\n",
    "!~/helm delete --purge train-dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config file for the package.\n",
    "eval_config = {\n",
    "    \"image\": \"gcr.io/deepvariant-docker/deepvariant:0.4.0\",    \n",
    "    \"dataset_config_pbtxt\": GCS_DATASET_CONFIG,\n",
    "    \"train_dir\": train_config[\"train_dir\"],\n",
    "    \"eval_dir\": os.path.join(GCS_DIR, \"eval\"),\n",
    "}\n",
    "    \n",
    "EVAL_CONFIG_FILE=\"/tmp/model_eval_config.yaml\"\n",
    "\n",
    "config_yaml = yaml.dump(eval_config, Dumper=ExplicitDumper, default_flow_style=False)\n",
    "\n",
    "with open(EVAL_CONFIG_FILE, \"w\") as hf:\n",
    "    hf.write(config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   train-eval\r\n",
      "LAST DEPLOYED: Mon Dec 11 22:19:19 2017\r\n",
      "NAMESPACE: default\r\n",
      "STATUS: DEPLOYED\r\n",
      "\r\n",
      "RESOURCES:\r\n",
      "==> v1alpha1/TfJob\r\n",
      "NAME        AGE\r\n",
      "train-eval  0s\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!~/helm install --name=train-eval ./charts/dv2-eval/ -f {EVAL_CONFIG_FILE}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
